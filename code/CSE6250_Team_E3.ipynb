{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j01aH0PR4Sg-"
      },
      "source": [
        "# Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Draft/Final Report. It only considers the general type of research in our paper pool:\n",
        "\n",
        "1. Define a predictive problem in healthcare.\n",
        "2. Propose a data/feature engineering method to process (MIMIC III or other) datasets.\n",
        "3. Propose a neural network and train it supervisely.\n",
        "4. Compare the model with other deep learning models.\n",
        "5. Mostly use Python as the coding language. *(No worries if your coding language is not python, Colab is backended with Linux server and can be easily setup to support all kinds of coding lanuague, there are even code templates, you just need to make some serach online)*\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlv6knX04FiY"
      },
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
        "\n",
        "*   Background of the problem\n",
        "  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
        "  * what is the importance/meaning of solving the problem\n",
        "  * what is the difficulty of the problem\n",
        "  * the state of the art methods and effectiveness.\n",
        "*   Paper explanation\n",
        "  * what did the paper propose\n",
        "  * what is the innovations of the method\n",
        "  * how well the proposed method work (in its own metrics)\n",
        "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "outputs": [],
      "source": [
        "# code comment is used as inline annotations for your coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: xxxxxxx\n",
        "2.   Hypothesis 2: xxxxxxx\n",
        "\n",
        "You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n",
        "\n",
        "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM4WUjz64C3B"
      },
      "source": [
        "\n",
        "You can also use code to display images, see the code below.\n",
        "\n",
        "The images must be saved in Google Drive first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRksCB1vbYwJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/multimodal-clinical-outcome')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "outputs": [],
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "#from google.colab import drive\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset,Dataset\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
        "from plots import plot_learning_curves, plot_confusion_matrix\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "from operator import itemgetter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODE = \"BOTH\"  # Options: 'BOTH', 'TRAIN', 'TEST'\n",
        "TARGET_VARIABLES = [\"los_3\", \"los_7\"]  # Options: 'mort_hosp', 'mort_icu', 'los_3', 'los_7'\n",
        "NUMBER_OF_WORKERS = 0\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0001\n",
        "SIGMOID_THRESHOLD = 0.5\n",
        "\n",
        "DATASET_FILE_PATH = \"../output\"\n",
        "PATH_OUTPUT = \"../output/model/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_FILE_PATH = \"../output\"\n",
        "PATH_OUTPUT = \"../output/model/\"\n",
        "\n",
        "def read_data():\n",
        "\ttrain_X = pd.read_pickle(f\"{DATASET_FILE_PATH}/train_features.pkl\")\n",
        "\ttrain_Y = pd.read_pickle(f\"{DATASET_FILE_PATH}/train_labels.pkl\")\n",
        "\n",
        "\tvalidation_X = pd.read_pickle(f\"{DATASET_FILE_PATH}/validation_features.pkl\")\n",
        "\tvalidation_Y = pd.read_pickle(f\"{DATASET_FILE_PATH}/validation_labels.pkl\")\n",
        "\n",
        "\ttest_X = pd.read_pickle(f\"{DATASET_FILE_PATH}/test_features.pkl\")\n",
        "\ttest_Y = pd.read_pickle(f\"{DATASET_FILE_PATH}/test_labels.pkl\")\n",
        "\n",
        "\tpatient_embed = pd.read_pickle(f\"{DATASET_FILE_PATH}/patient_embeddings.pkl\")\n",
        "  \n",
        "\n",
        "\treturn (train_X, train_Y), (validation_X, validation_Y), (test_X, test_Y), patient_embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and Collate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "\tdef __init__(self,seqs,embeddings,labels):\n",
        "\n",
        "\t\tif len(seqs) != len(labels):\n",
        "\t\t\traise ValueError(\"Seqs and Labels have different lengths\")\n",
        "\t\tself.labels = labels\n",
        "\t\tself.seqs = seqs #  torch view 24 X 104\n",
        "\t\tself.embed = embeddings\n",
        "\t\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.labels)\n",
        "\t\n",
        "\tdef __getitem__(self,index):\n",
        "\t\treturn self.embed[index],self.seqs[index], self.labels[index]\n",
        "\n",
        "def collate_embeddings(batch):\n",
        "\t\"\"\"\n",
        "\t'batch' is a list [(embed_1,labs_1, label_1), (embed_2,labs_2, label_2), ... , (embed_N,labs_N, label_N)]\n",
        "\t:returns \n",
        "\t\tseqs (FloatTensor) - 3D BACTCH SIZE X max_lenght X num_features(100=dim embeddings)\n",
        "\t\tlenghts(LongTensor) - 1D of batch size\n",
        "\t\tlabels (LongTensor) - 1D of batch size\n",
        "\t\"\"\"\n",
        "\tmax_len = max([seq[0].shape[0] for seq in batch])\n",
        "\tnew_seqs=[]\n",
        "\tfor seq in batch:\n",
        "\t\tlabs = seq[1]\n",
        "\t\tlabel = seq[2]\n",
        "\t\tlength_ = seq[0].shape[0]\n",
        "\t\tpad = np.zeros((max_len-seq[0].shape[0],seq[0].shape[1]))\n",
        "\t\tnew_seq = np.concatenate((seq[0], pad), axis=0)\n",
        "\n",
        "\t\tnew_seqs.append((new_seq,length_,labs,label))\n",
        "\t\t\n",
        "\t#sorted list for the batch\t\t\n",
        "\tnew_sorted_seqs = sorted(new_seqs, key=itemgetter(1),reverse=True)\n",
        "\t\n",
        "\n",
        "\tembed_tensor = torch.FloatTensor(np.array([seq[0] for seq in new_sorted_seqs]))\n",
        "\tlabs_tensor = torch.stack([seq[2] for seq in new_sorted_seqs])\n",
        "\tlabels_tensor = torch.LongTensor(np.array([seq[3] for seq in new_sorted_seqs]))\n",
        "\n",
        "\treturn (embed_tensor,labs_tensor),labels_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def dataframe_to_tensorDataset(train, validation, test, targetVariable,embeddings):\n",
        "\n",
        "\tpatient_ids_with_embeddings = [id for id in embeddings.SUBJECT_ID]\n",
        "\n",
        "\t#print(\"# patients with ids:\",len(patient_ids_with_embeddings))\n",
        "\n",
        "\ttrain_X, train_Y = train\n",
        "\tvalidation_X, validation_Y = validation\n",
        "\ttest_X, test_Y = test\n",
        "\n",
        "\t# Only keepinf patients with embeddings\n",
        "\ttrain_X = train_X[train_X.index.get_level_values(\"subject_id\").isin(patient_ids_with_embeddings)]\n",
        "\ttrain_Y = train_Y[train_Y.index.get_level_values(\"subject_id\").isin(patient_ids_with_embeddings)]\n",
        "\n",
        "\tvalidation_X = validation_X[validation_X.index.get_level_values(\"subject_id\").isin(patient_ids_with_embeddings)]\n",
        "\tvalidation_Y = validation_Y[validation_Y.index.get_level_values(\"subject_id\").isin(patient_ids_with_embeddings)]\n",
        "\n",
        "\ttest_X = test_X[test_X.index.get_level_values(\"subject_id\").isin(patient_ids_with_embeddings)]\n",
        "\ttest_Y = test_Y[test_Y.index.get_level_values(\"subject_id\").isin(patient_ids_with_embeddings)]\n",
        "\n",
        "\ttrain_embed = embeddings[embeddings['SUBJECT_ID'].isin(train_X.index.get_level_values(\"subject_id\"))]\n",
        "\tvalidation_embed = embeddings[embeddings['SUBJECT_ID'].isin(validation_X.index.get_level_values(\"subject_id\"))]\n",
        "\ttest_embed = embeddings[embeddings['SUBJECT_ID'].isin(test_X.index.get_level_values(\"subject_id\"))]\n",
        "\t\n",
        "\t# data_train = torch.tensor(train_X.values, dtype=torch.float32).view(-1, 24, 104)\n",
        "\t#print(\"train shape:\",data_train.shape)\n",
        "\t#print(\"train index=\",data_train[0].shape)\n",
        "\t#print(len(set(train_X.index.get_level_values(\"subject_id\"))))\n",
        "\n",
        "\ttrain = TimeSeriesDataset(seqs = torch.tensor(train_X.values, dtype=torch.float32).view(-1, 24, 104),\n",
        "\t\t\t\t\t\t\t  embeddings = train_embed['word2vec'].tolist(),\n",
        "\t\t\t\t\t\t\t\tlabels = torch.tensor(train_Y[targetVariable].values))\n",
        "\tvalidation = TimeSeriesDataset(seqs = torch.tensor(validation_X.values, dtype=torch.float32).view(-1, 24, 104),\n",
        "\t\t\t\t\t\t\t  embeddings= validation_embed['word2vec'].tolist(),\n",
        "\t\t\t\t\t\t\t\tlabels = torch.tensor(validation_Y[targetVariable].values))\n",
        "\ttest = TimeSeriesDataset(seqs = torch.tensor(test_X.values, dtype=torch.float32).view(-1, 24, 104),\n",
        "\t\t\t\t\t\t\t  embeddings= test_embed['word2vec'].tolist(),\n",
        "\t\t\t\t\t\t\t\tlabels = torch.tensor(test_Y[targetVariable].values))\n",
        "\n",
        "\n",
        "\t# train = TensorDataset(torch.tensor(train_X.values, dtype=torch.float32).view(-1, 24, 104),\n",
        "\t#                       torch.tensor(train_Y[targetVariable].values))\n",
        "\t# validation = TensorDataset(torch.tensor(validation_X.values, dtype=torch.float32).view(-1, 24, 104),\n",
        "\t#                            torch.tensor(validation_Y[targetVariable].values))\n",
        "\t# test = TensorDataset(torch.tensor(test_X.values, dtype=torch.float32).view(-1, 24, 104),\n",
        "\t#                      torch.tensor(test_Y[targetVariable].values))\n",
        "\n",
        "\treturn train, validation, test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# My Model\n",
        "class ConvolutionalNERGRU(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(ConvolutionalNERGRU, self).__init__()\n",
        "\n",
        "\t\tself.conv1 = nn.Conv1d(in_channels=100,out_channels=32,kernel_size=3,padding=2,stride=1)\n",
        "\t\tself.conv2 = nn.Conv1d(in_channels=32,out_channels=64,kernel_size=3,padding=2,stride=1)\n",
        "\t\tself.conv3 = nn.Conv1d(in_channels=64,out_channels=96,kernel_size=3,padding=2,stride=1)\n",
        "\t\tself.globalpooling= nn.AdaptiveMaxPool1d(1)\n",
        "\t\tself.flatten = nn.Flatten()\n",
        "\t\tself.dropout1= nn.Dropout(0.2)\n",
        "\n",
        "\t\tself.gru = nn.GRU(104, 256, dropout=0.2, batch_first=True)\n",
        "\t\tself.sigmoid = nn.ReLU()\n",
        "\t\tself.hiddenLayer = nn.Linear(352, 2)\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\tembed,seqs = input\n",
        "\t\tout_gru, _ = self.gru(seqs)\n",
        "\n",
        "\n",
        "\t\t#cnn takes input of shape (batch_size, channels, seq_len)\n",
        "\t\tout_embed = embed.permute(0,2,1)\n",
        "\t\tout_embed = F.relu(self.conv1(out_embed))\n",
        "\t\tout_embed = F.relu(self.conv2(out_embed))\n",
        "\t\tout_embed = F.relu(self.conv3(out_embed))\n",
        "\t\tout_embed = self.globalpooling(out_embed)\n",
        "\t\tout_embed = self.flatten(out_embed)\n",
        "\n",
        "\t\toutput = torch.concat((out_embed,out_gru[:,-1,:]),dim=1)\n",
        "\n",
        "\t\toutput = self.hiddenLayer(output)\n",
        "\t\toutput = self.sigmoid(output)\n",
        "\n",
        "\t\treturn output\n",
        "\t\n",
        "def sigmoid_predict(output):\n",
        "\tresults = []\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\tfor data in output:\n",
        "\t\t\tresults.append(int(data[1] > SIGMOID_THRESHOLD))\n",
        "\n",
        "\treturn torch.tensor(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#\n",
        "# Citation: Jimeng Sun, (2024). CSE6250: Big Data Analytics in Healthcare Homework 4\n",
        "# Used code from utils.py\n",
        "#\n",
        "class Metrics:\n",
        "\tdef __init__(self):\n",
        "\t\tself.value = 0\n",
        "\t\tself.average = 0\n",
        "\t\tself.sum = 0\n",
        "\t\tself.count = 0\n",
        "\n",
        "\tdef update(self, value, n=1):\n",
        "\t\tself.value = value\n",
        "\t\tself.sum += self.value * n\n",
        "\t\tself.count += n\n",
        "\t\tself.average = self.sum / self.count\n",
        "\n",
        "\n",
        "def calculate_accuracy(predicted, target):\n",
        "\twith torch.no_grad():\n",
        "\t\tbatchSize = target.size(0)\n",
        "\t\tcorrect = predicted.eq(target).sum().item()\n",
        "\n",
        "\t\treturn (correct / batchSize) * 100.0\n",
        "\n",
        "\n",
        "#\n",
        "# Citation: Jimeng Sun, (2024). CSE6250: Big Data Analytics in Healthcare Homework 4\n",
        "# Used code from utils.py and train_seizure.py\n",
        "#\n",
        "def train_model(model, device, dataLoader, criterion, optimizer, epoch):\n",
        "\tbatchTime = Metrics()\n",
        "\tdataTime = Metrics()\n",
        "\tlosses = Metrics()\n",
        "\taccuracy = Metrics()\n",
        "\n",
        "\tmodel.train()\n",
        "\n",
        "\tend = time.time()\n",
        "\n",
        "\tfor i, (input, target) in enumerate(dataLoader):\n",
        "\t\tdataTime.update(time.time() - end)\n",
        "\n",
        "\t\t#input = input.to(device)\n",
        "\t\tif isinstance(input, tuple):\n",
        "\t\t\tinput = tuple([e.to(device) if type(e) == torch.Tensor else e for e in input])\n",
        "\t\telse:\n",
        "\t\t\tinput = input.to(device)\n",
        "\t\ttarget = target.to(device)\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\toutput = model(input)\n",
        "\t\tloss = criterion(output, target)\n",
        "\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\tbatchTime.update(time.time() - end)\n",
        "\t\tend = time.time()\n",
        "\n",
        "\t\tlosses.update(loss.item(), target.size(0))\n",
        "\t\taccuracy.update(calculate_accuracy(sigmoid_predict(output), target), target.size(0))\n",
        "\n",
        "\t\tprint(f'Epoch: [{epoch}][{i}/{len(dataLoader)}]\\t'\n",
        "\t\t      f'Time {batchTime.value:.3f} ({batchTime.average:.3f})\\t'\n",
        "\t\t      f'Data {dataTime.value:.3f} ({dataTime.average:.3f})\\t'\n",
        "\t\t      f'Loss {losses.value:.4f} ({losses.average:.4f})\\t'\n",
        "\t\t      f'Accuracy {accuracy.value:.3f} ({accuracy.average:.3f})')\n",
        "\n",
        "\treturn losses.average, accuracy.average\n",
        "\n",
        "\n",
        "#\n",
        "# Citation: Jimeng Sun, (2024). CSE6250: Big Data Analytics in Healthcare Homework 4\n",
        "# Used code from utils.py and train_seizure.py\n",
        "#\n",
        "def test_model(model, device, dataLoader, criterion):\n",
        "\tbatchTime = Metrics()\n",
        "\tlosses = Metrics()\n",
        "\taccuracy = Metrics()\n",
        "\n",
        "\tresults = []\n",
        "\n",
        "\tmodel.eval()\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\tend = time.time()\n",
        "\n",
        "\t\tfor i, (input, target) in enumerate(dataLoader):\n",
        "\t\t\t#input = input.to(device)\n",
        "\n",
        "\t\t\tif isinstance(input, tuple):\n",
        "\t\t\t\tinput = tuple([e.to(device) if type(e) == torch.Tensor else e for e in input])\n",
        "\t\t\telse:\n",
        "\t\t\t\tinput = input.to(device)\n",
        "\t\t\ttarget = target.to(device)\n",
        "\n",
        "\t\t\toutput = model(input)\n",
        "\t\t\tloss = criterion(output, target)\n",
        "\n",
        "\t\t\tbatchTime.update(time.time() - end)\n",
        "\t\t\tend = time.time()\n",
        "\n",
        "\t\t\tlosses.update(loss.item(), target.size(0))\n",
        "\n",
        "\t\t\tpredicted = sigmoid_predict(output)\n",
        "\t\t\taccuracy.update(calculate_accuracy(predicted, target), target.size(0))\n",
        "\n",
        "\t\t\ttrue = target.detach().cpu().numpy().tolist()\n",
        "\t\t\tpredicted = predicted.detach().cpu().numpy().tolist()\n",
        "\t\t\tresults.extend(list(zip(true, predicted)))\n",
        "\n",
        "\t\t\tprint(f'Test: [{i}/{len(dataLoader)}]\\t'\n",
        "\t\t\t      f'Time {batchTime.value:.3f} ({batchTime.average:.3f})\\t'\n",
        "\t\t\t      f'Loss {losses.value:.4f} ({losses.average:.4f})\\t'\n",
        "\t\t\t      f'Accuracy {accuracy.value:.3f} ({accuracy.average:.3f})')\n",
        "\n",
        "\treturn losses.average, accuracy.average, results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "# Citation: Jimeng Sun, (2024). CSE6250: Big Data Analytics in Healthcare Homework 4\n",
        "# Used code from train_seizure.py\n",
        "#\n",
        "def model_runner(train, validation, test, targetVariable):\n",
        "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\t# model = GRU()\n",
        "\tmodel = ConvolutionalNERGRU()\n",
        "\tmodel.to(device)\n",
        "\n",
        "\tcriterion = nn.CrossEntropyLoss()\n",
        "\t#criterion = nn.BCEWithLogitsLoss()\n",
        "\tcriterion.to(device)\n",
        "\n",
        "\tif MODE.upper() == \"BOTH\" or MODE.upper() == \"TRAIN\":\n",
        "\t\ttrainLoader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_embeddings,num_workers=NUMBER_OF_WORKERS)\n",
        "\t\tvalidationLoader = DataLoader(validation, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_embeddings,num_workers=NUMBER_OF_WORKERS)\n",
        "\n",
        "\t\toptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\t\tbestValidationAccuracy = 0.0\n",
        "\t\ttrainLosses, trainAccuracies = [], []\n",
        "\t\tvalidationLosses, validationAccuracies = [], []\n",
        "\n",
        "\t\tfor epoch in range(EPOCHS):\n",
        "\t\t\ttrainLoss, trainAccuracy = train_model(model, device, trainLoader, criterion, optimizer, epoch)\n",
        "\t\t\tvalidationLoss, validationAccuracy, _ = test_model(model, device, validationLoader, criterion)\n",
        "\n",
        "\t\t\ttrainLosses.append(trainLoss)\n",
        "\t\t\tvalidationLosses.append(validationLoss)\n",
        "\n",
        "\t\t\ttrainAccuracies.append(trainAccuracy)\n",
        "\t\t\tvalidationAccuracies.append(validationAccuracy)\n",
        "\n",
        "\t\t\tif validationAccuracy > bestValidationAccuracy:\n",
        "\t\t\t\tbestValidationAccuracy = validationAccuracy\n",
        "\t\t\t\ttorch.save(model, os.path.join(PATH_OUTPUT, f\"{targetVariable}_GRU.pth\"))\n",
        "\n",
        "\t\tplot_learning_curves(trainLosses, validationLosses, trainAccuracies, validationAccuracies,\n",
        "\t\t\t\t\t\t\t f\"{targetVariable.upper()} GRU\")\n",
        "\n",
        "\tif MODE.upper() == \"BOTH\" or MODE.upper() == \"TEST\":\n",
        "\t\ttestLoader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_embeddings,num_workers=NUMBER_OF_WORKERS)\n",
        "\n",
        "\t\tbestModel = torch.load(os.path.join(PATH_OUTPUT, f\"{targetVariable}_GRU.pth\"))\n",
        "\t\t_, _, testResults = test_model(bestModel, device, testLoader, criterion)\n",
        "\n",
        "\t\ty_true, y_pred = zip(*testResults)\n",
        "\n",
        "\t\taurocScore = roc_auc_score(y_true, y_pred)\n",
        "\t\tauprcScore = average_precision_score(y_true, y_pred)\n",
        "\t\tf1Score = f1_score(y_true, y_pred)\n",
        "\n",
        "\t\tprint('\\nFinal Test scores: \\n'\n",
        "\t\t\t  f'{targetVariable} AUROC Score: {aurocScore}\\n'\n",
        "\t\t\t  f'{targetVariable} AUPRC Score: {auprcScore}\\n'\n",
        "\t\t\t  f'{targetVariable} F1 Score: {f1Score}')\n",
        "\n",
        "\t\tplot_confusion_matrix(y_true, y_pred, [\"No\", \"Yes\"], f\"{targetVariable.upper()} GRU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mayerantoine/miniforge3/envs/cs6250-multimodal/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/20]\tTime 0.251 (0.251)\tData 0.011 (0.011)\tLoss 0.7196 (0.7196)\tAccuracy 46.875 (46.875)\n",
            "Epoch: [0][1/20]\tTime 0.120 (0.186)\tData 0.005 (0.008)\tLoss 0.6915 (0.7055)\tAccuracy 53.125 (50.000)\n",
            "Epoch: [0][2/20]\tTime 0.152 (0.174)\tData 0.015 (0.010)\tLoss 0.6989 (0.7033)\tAccuracy 50.000 (50.000)\n",
            "Epoch: [0][3/20]\tTime 0.179 (0.175)\tData 0.004 (0.008)\tLoss 0.7106 (0.7052)\tAccuracy 71.875 (55.469)\n",
            "Epoch: [0][4/20]\tTime 0.057 (0.152)\tData 0.004 (0.008)\tLoss 0.6768 (0.6995)\tAccuracy 59.375 (56.250)\n",
            "Epoch: [0][5/20]\tTime 0.079 (0.140)\tData 0.002 (0.007)\tLoss 0.6766 (0.6957)\tAccuracy 65.625 (57.812)\n",
            "Epoch: [0][6/20]\tTime 0.101 (0.134)\tData 0.003 (0.006)\tLoss 0.6930 (0.6953)\tAccuracy 59.375 (58.036)\n",
            "Epoch: [0][7/20]\tTime 0.123 (0.133)\tData 0.003 (0.006)\tLoss 0.6823 (0.6937)\tAccuracy 68.750 (59.375)\n",
            "Epoch: [0][8/20]\tTime 0.076 (0.126)\tData 0.002 (0.005)\tLoss 0.7328 (0.6980)\tAccuracy 62.500 (59.722)\n",
            "Epoch: [0][9/20]\tTime 0.087 (0.123)\tData 0.003 (0.005)\tLoss 0.7277 (0.7010)\tAccuracy 46.875 (58.438)\n",
            "Epoch: [0][10/20]\tTime 0.097 (0.120)\tData 0.003 (0.005)\tLoss 0.6768 (0.6988)\tAccuracy 40.625 (56.818)\n",
            "Epoch: [0][11/20]\tTime 0.112 (0.119)\tData 0.003 (0.005)\tLoss 0.6899 (0.6980)\tAccuracy 56.250 (56.771)\n",
            "Epoch: [0][12/20]\tTime 0.078 (0.116)\tData 0.002 (0.004)\tLoss 0.6839 (0.6970)\tAccuracy 46.875 (56.010)\n",
            "Epoch: [0][13/20]\tTime 0.083 (0.114)\tData 0.002 (0.004)\tLoss 0.7233 (0.6988)\tAccuracy 53.125 (55.804)\n",
            "Epoch: [0][14/20]\tTime 0.101 (0.113)\tData 0.002 (0.004)\tLoss 0.7009 (0.6990)\tAccuracy 50.000 (55.417)\n",
            "Epoch: [0][15/20]\tTime 0.088 (0.111)\tData 0.002 (0.004)\tLoss 0.6688 (0.6971)\tAccuracy 65.625 (56.055)\n",
            "Epoch: [0][16/20]\tTime 0.080 (0.110)\tData 0.001 (0.004)\tLoss 0.6790 (0.6960)\tAccuracy 59.375 (56.250)\n",
            "Epoch: [0][17/20]\tTime 0.094 (0.109)\tData 0.002 (0.004)\tLoss 0.6513 (0.6935)\tAccuracy 62.500 (56.597)\n",
            "Epoch: [0][18/20]\tTime 0.077 (0.107)\tData 0.002 (0.004)\tLoss 0.6825 (0.6930)\tAccuracy 50.000 (56.250)\n",
            "Epoch: [0][19/20]\tTime 0.058 (0.105)\tData 0.001 (0.004)\tLoss 0.6338 (0.6910)\tAccuracy 66.667 (56.598)\n",
            "Test: [0/3]\tTime 0.032 (0.032)\tLoss 0.6805 (0.6805)\tAccuracy 50.000 (50.000)\n",
            "Test: [1/3]\tTime 0.040 (0.036)\tLoss 0.6654 (0.6729)\tAccuracy 59.375 (54.688)\n",
            "Test: [2/3]\tTime 0.049 (0.041)\tLoss 0.7112 (0.6837)\tAccuracy 60.000 (56.180)\n",
            "Test: [0/6]\tTime 0.038 (0.038)\tLoss 0.6465 (0.6465)\tAccuracy 56.250 (56.250)\n",
            "Test: [1/6]\tTime 0.034 (0.036)\tLoss 0.6655 (0.6560)\tAccuracy 59.375 (57.812)\n",
            "Test: [2/6]\tTime 0.030 (0.034)\tLoss 0.6465 (0.6528)\tAccuracy 56.250 (57.292)\n",
            "Test: [3/6]\tTime 0.059 (0.040)\tLoss 0.6854 (0.6610)\tAccuracy 50.000 (55.469)\n",
            "Test: [4/6]\tTime 0.055 (0.043)\tLoss 0.6810 (0.6650)\tAccuracy 65.625 (57.500)\n",
            "Test: [5/6]\tTime 0.035 (0.042)\tLoss 0.6752 (0.6659)\tAccuracy 62.500 (57.955)\n",
            "\n",
            "Final Test scores: \n",
            "los_3 AUROC Score: 0.5066666666666667\n",
            "los_3 AUPRC Score: 0.4337878787878788\n",
            "los_3 F1 Score: 0.02631578947368421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mayerantoine/miniforge3/envs/cs6250-multimodal/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/20]\tTime 0.081 (0.081)\tData 0.002 (0.002)\tLoss 0.7591 (0.7591)\tAccuracy 90.625 (90.625)\n",
            "Epoch: [0][1/20]\tTime 0.100 (0.090)\tData 0.009 (0.005)\tLoss 0.7636 (0.7614)\tAccuracy 84.375 (87.500)\n",
            "Epoch: [0][2/20]\tTime 0.121 (0.101)\tData 0.004 (0.005)\tLoss 0.7025 (0.7417)\tAccuracy 87.500 (87.500)\n",
            "Epoch: [0][3/20]\tTime 0.100 (0.100)\tData 0.003 (0.004)\tLoss 0.7136 (0.7347)\tAccuracy 78.125 (85.156)\n",
            "Epoch: [0][4/20]\tTime 0.098 (0.100)\tData 0.002 (0.004)\tLoss 0.6669 (0.7211)\tAccuracy 96.875 (87.500)\n",
            "Epoch: [0][5/20]\tTime 0.081 (0.097)\tData 0.002 (0.004)\tLoss 0.6903 (0.7160)\tAccuracy 96.875 (89.062)\n",
            "Epoch: [0][6/20]\tTime 0.080 (0.094)\tData 0.003 (0.003)\tLoss 0.6701 (0.7094)\tAccuracy 96.875 (90.179)\n",
            "Epoch: [0][7/20]\tTime 0.082 (0.093)\tData 0.002 (0.003)\tLoss 0.6309 (0.6996)\tAccuracy 93.750 (90.625)\n",
            "Epoch: [0][8/20]\tTime 0.099 (0.093)\tData 0.004 (0.003)\tLoss 0.6594 (0.6951)\tAccuracy 90.625 (90.625)\n",
            "Epoch: [0][9/20]\tTime 0.101 (0.094)\tData 0.004 (0.003)\tLoss 0.5596 (0.6816)\tAccuracy 100.000 (91.562)\n",
            "Epoch: [0][10/20]\tTime 0.097 (0.094)\tData 0.001 (0.003)\tLoss 0.6289 (0.6768)\tAccuracy 93.750 (91.761)\n",
            "Epoch: [0][11/20]\tTime 0.070 (0.092)\tData 0.002 (0.003)\tLoss 0.6035 (0.6707)\tAccuracy 90.625 (91.667)\n",
            "Epoch: [0][12/20]\tTime 0.161 (0.098)\tData 0.003 (0.003)\tLoss 0.6044 (0.6656)\tAccuracy 93.750 (91.827)\n",
            "Epoch: [0][13/20]\tTime 0.078 (0.096)\tData 0.002 (0.003)\tLoss 0.5593 (0.6580)\tAccuracy 93.750 (91.964)\n",
            "Epoch: [0][14/20]\tTime 0.086 (0.096)\tData 0.006 (0.003)\tLoss 0.4685 (0.6454)\tAccuracy 100.000 (92.500)\n",
            "Epoch: [0][15/20]\tTime 0.080 (0.095)\tData 0.002 (0.003)\tLoss 0.4976 (0.6361)\tAccuracy 100.000 (92.969)\n",
            "Epoch: [0][16/20]\tTime 0.108 (0.095)\tData 0.003 (0.003)\tLoss 0.5367 (0.6303)\tAccuracy 90.625 (92.831)\n",
            "Epoch: [0][17/20]\tTime 0.069 (0.094)\tData 0.002 (0.003)\tLoss 0.5410 (0.6253)\tAccuracy 87.500 (92.535)\n",
            "Epoch: [0][18/20]\tTime 0.202 (0.100)\tData 0.002 (0.003)\tLoss 0.5145 (0.6195)\tAccuracy 96.875 (92.763)\n",
            "Epoch: [0][19/20]\tTime 0.175 (0.103)\tData 0.001 (0.003)\tLoss 0.4859 (0.6150)\tAccuracy 95.238 (92.846)\n",
            "Test: [0/3]\tTime 0.030 (0.030)\tLoss 0.4791 (0.4791)\tAccuracy 90.625 (90.625)\n",
            "Test: [1/3]\tTime 0.039 (0.035)\tLoss 0.4790 (0.4791)\tAccuracy 96.875 (93.750)\n",
            "Test: [2/3]\tTime 0.047 (0.039)\tLoss 0.4985 (0.4845)\tAccuracy 92.000 (93.258)\n",
            "Test: [0/6]\tTime 0.033 (0.033)\tLoss 0.4751 (0.4751)\tAccuracy 93.750 (93.750)\n",
            "Test: [1/6]\tTime 0.036 (0.034)\tLoss 0.5165 (0.4958)\tAccuracy 87.500 (90.625)\n",
            "Test: [2/6]\tTime 0.031 (0.033)\tLoss 0.4675 (0.4864)\tAccuracy 96.875 (92.708)\n",
            "Test: [3/6]\tTime 0.056 (0.039)\tLoss 0.5327 (0.4980)\tAccuracy 87.500 (91.406)\n",
            "Test: [4/6]\tTime 0.054 (0.042)\tLoss 0.4690 (0.4922)\tAccuracy 96.875 (92.500)\n",
            "Test: [5/6]\tTime 0.038 (0.041)\tLoss 0.4796 (0.4910)\tAccuracy 93.750 (92.614)\n",
            "\n",
            "Final Test scores: \n",
            "los_7 AUROC Score: 0.5\n",
            "los_7 AUPRC Score: 0.07386363636363637\n",
            "los_7 F1 Score: 0.0\n"
          ]
        }
      ],
      "source": [
        "trainSet, validationSet, testSet, embeddings = read_data()\n",
        "for targetVariable in TARGET_VARIABLES:\n",
        "\tnewTrainSet, newValidationSet, newTestSet = dataframe_to_tensorDataset(trainSet, validationSet, testSet,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   targetVariable,embeddings)\n",
        "\t\t\n",
        "\tmodel_runner(newTrainSet, newValidationSet, newTestSet, targetVariable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "outputs": [],
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Model comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "outputs": [],
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "outputs": [],
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1.   Sun, J, [paper title], [journal title], [year], [volume]:[issue], doi: [doi link to paper]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmVuzQ724HbO"
      },
      "source": [
        "# Feel free to add new sections"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
